CS323- Artificial Intelligence Notes

September 15 to September 25

by,

Archit Dwivedi

B18BB005

Previously we saw how modelling a relaxed version of the problem can pave our way for the discovery of an admissible heuristic. We depicted the paradigm mentioned above, using the 8 Puzzle problem. There, we relaxed the movement of each tile as if there were no other tile except that. By doing so, our heuristic got reduced to the Manhattan distance. That is one step in the right direction as it would not solve the problem but make it relatively easier to solve it (using a machine). We might have to do more computation, but we are not primarily concerned about the computational expenses. We just want a good approximation. At this point, one might question why we are so keen on learning an approximation and not just use the actual cost right away. It will be an admissible heuristic which would lead us straight to the goal hence saving our computation cost on the node expansion on a suboptimal path. But that is no compensation on the computational expenditure on the calculation of actual cost itself. The rule of thumb is- better the heuristic, fewer the expansion of the tree.

Since an admissible heuristic is a lower bound on the actual cost, a different heuristic which returns even smaller approximations would also be admissible. On top of that, considering there are no negative costs involved, zero is a trivial heuristic with which A* algorithm would be no different than the UCS. Starting from the trivial heuristic up to the exact heuristic, we can have a family of considerable heuristics. There can be a range from really bad to really good heuristics. But the best ones reside somewhere in the middle. We have to analyse the problem and choose the heuristic based on the estimation/work-per-node tradeoff that can be afforded.

We have seen algorithms that search for a path from a source node to the goal through a tree. In the process, on many occasions, the algorithm expands an already expanded approach which is nothing but pure redundancy. We can save our time wasted on exploring an already explored corner of the tree by simply tweaking the algorithm to skip the nodes that it has already visited. For this purpose, we maintain a closed set to store the information whether a node has been expanded yet or not. We use a closed set instead of a simple linked list because a graph can have a large number of nodes which would increase the time complexity of the solution if it is stored in a linked list. This is the last thing we want to have when the whole purpose of this exercise is to reduce the amount of time wasted in exploring the already explored nodes. Now we explore the tree node by node but before expanding the path through a particular node, check if it has already been expanded. If not, then expand it otherwise skip the node.

Let’s consider the A* graph search algorithm. Assume there are two nodes in the fringe with different heuristic values and same backward costs. Let the actual cost to a goal from the node with larger heuristic value is smaller than the actual cost from the node with smaller heuristic value. Since both the nodes in the fringe have the same backward cost, our algorithm will pursue the node which has smaller heuristic value but, as we know it, the larger actual cost. Because we have been maintaining a closed set of visited nodes, the common ancestor of those nodes would not be explored again in the future either. In this way, A* graph algorithm fails to find an optimal solution. Our heuristic was admissible, but just because there was an inconsistency in the heuristic values, A* graph algorithm gave us a suboptimal solution.

Let’s try to reverse engineer the faults in the A* graph search algorithm considering the case using a small graph with three nodes- A with heuristic value 4, C with heuristic value one and G with heuristic value 0. Edges being- A->C with cost of 1 and C->G with cost 3. According to the heuristic, it should cost 1 unit to go from C to G. If we are at C, we know A->C cost 1 unit and using heuristic we are estimating C->G jump should cost 1 unit. That makes the heuristic cost of A->G (which is 4) way too large than what it may seem after reaching C (which is 2). It happened because of inconsistency in heuristic values of node A and C. Therefore, in extension to admissible condition, we apply an additional constraint to our heuristic function that can make it consistent across the nodes. The condition is: actual-cost(A->C)+h(c) >= h(A).

Because it involved actual cost, it already implies admissibility (that is, actual-cost(A->G) >= h(A)). In other words, a consistent heuristic would never be the overestimate of each arc cost.

Consequently, the f value would never decrease along the path. Since f value never decreases, an optimal path will be explored prior to any other suboptimal path. That makes A* graph search to always return optimal solution.

To sum up, in all algorithms that we have studied until now, we have seen two significant categories of search problems- informed and uninformed search problems. In both, we modelled the world based on some assumptions like there should be only one agent that can take some actions due to which its state can change. What we have dealt with until now were the problems where our solution is a path from the source state to the goal test. In most cases, there were more than one paths to the goal test. In such cases, we concern ourselves with finding the optimal solution. In the informed search, we used the heuristic function, which gave us an estimate of how far a state is from the goal test. All these are the characteristics of planning problems- where the sequences of actions matter. There is another class of problems called identification problems. Here, the sequence of the actions has little to no value. Only goal tests are important regardless of how we reach there. CSP (Constraint Satisfaction Problem) falls into this class of problems.

When we compare standard search problems with CSPs, we find that in the usual search problem, we had states that represented various configurations of the world. They can be represented using any data structure as far as the computer is able to distinguish whether the agent is standing on a goal state or not. Also, there was a successor function using which we used to implement our search strategy on the graph representation of our world. While in CSPs, we have a set of variables that describes our states that take values from a defined domain. Depending on the requirements of the problem, different variables may have the same or different domains. We are also provided with a set of constraints in the problem which are needed to be satisfied while assigning values to the variables.

One famous example one may see in the literature in the context of CSP is the map-colouring problem. In the problem, we are given a map which is separated into some number of contiguous regions. Given a set of colours, we have to colour each region such that no adjacent regions should share a common colour. This problem can be formulated as a constraint satisfaction problem.

For illustration purpose, suppose we are interested in colouring the map of Australia in the said manner. Australia has seven states which are needed to be coloured in either red, blue or green. So we declare seven variables, one for each state. Variables: WA, NT, Q, NSW, V, SA and T. Our variable can take a value corresponding to the colours it can have. So the domain for our variables can be declared as- {red, green, blue}. We are supposed to assign a colour to our seven variables such that no adjacent state has the same value. This is our constraint. During the formulation process, we can write the constraints in two forms- implicit or explicit. For example, we know WA and NT are adjacent states so they should not have the same value (WA != NT). If we write all such combinations in this manner, it is called an implicit representation of constraints. While it is called the explicit representation of the constraints if we write a set of all possible values that our variables can take, like- (WA, NT) ordered pair can have values in {(red, blue), (blue, red), (blue, green), (green, blue), (green, red), (red, green)} so on and so forth. In implicit representation, there is evaluation involved. We write a procedural way of verifying our constraints in a code snippet. While in the explicit form of representing the constraints, we are explicitly telling the computer everything that is allowed, so computation cost is saved. Our solution would be a combination of the values of our variable such that our constraints are satisfied.

Another example of CSP is the N-Queens problem where ‘N’ is the number of queens arranged on an NxN chessboard. The problem is to come up with a configuration of these N queens such that no queen attacks any other queen. Below are the two possible formulations of the problem as CSP.

Formulation 1: We take each square of the board as one variable, say, Xij (where I,j are any pair of integers from 1 to N*N). Each variable (square) can take two values corresponding to two states- either is-queen or empty-square. On that basis, we can define our domain as {0,1}. 0 means empty-square and one means is-queen. Values of the variables will tell us what’s going on in the corresponding square. We know that the queen can attack vertically, horizontally and diagonally. So considering how the queen works in the game of chess, we design our constraints that there is no more than one queen in a row and column and diagonal. Therefore, our constraints are-

In support of i,j and k - (Xij,Xik) should belong to {(0,0),(0,1),(1,0)} [takes care of row] 

In support of i,j and k - (Xij,Xkj) should belong to {(0,0),(0,1),(1,0)} [takes care of columns]

In support of i,j and k - (Xij,Xi+k j+k) should belong to {(0,0),(0,1),(1,0)} [takes care of upper diagonal]

In support of i,j and k - (Xij,Xi-k j-k) should belong to {(0,0),(0,1),(1,0)} [takes care of lower diagonal]

In extension of that, there is one more constraint on the number of queens, that is,

in support of i and j, sum(Xij) = N

If we don’t provide this constraint, the algorithm will assign all the variables with the value 0.

We can pose this problem as a CSP in yet another way. Previously we took each square as a variable. Let’s take each queen as variable and the index of the row at which it resides as the domain. So variables are- {Q1,Q2,Q3….QN}. Domain is {1,2,3,4,.....,N}. Likewise, constraints get modified to the following-

If represented implicitly: for all i and j, threatening(Qi, Qj) should be false.

If represented explicitly: (Q1, Q2) should belong to {(1,3), (1,4)...} so on and so forth.

Besides noting down variables, domain and constraints while formulating a problem as CSP, we often draw constraint graphs. In practice, in cases where we use some graph algorithm to solve a CSP, making constraint graphs can be helpful. Each variable of the graph is represented as a node. While the arcs are drawn based on how many other variables are affected by altering the value of one variable. The arcs depict that these connected variables are bound by each other with one or more constraints. If it is a binary CSP like the map-colouring problem, then one arc connects two nodes. Otherwise, if the problem has constraints that affect more than two variables, then to connect all the constrained variables, an auxiliary node is used as a junction. That auxiliary node does not hold any value and is merely there to join the affected nodes.

One such example of non-binary CSP is the Criptarithmatic problem. Suppose we have received a cryptic message as follows-

TWO + TWO = FOUR

Before going into formulating the CSP, we need to set some assumptions to clear the things up. First, we assume that ‘+’ here represents a simple addition operation. Second, all the non-distinct letters (like ‘O’) are the same, and all distinct letters are different. Fourth, we assume calculations are in base 10. Here, we can take each letter as one variable. Additionally, since we are doing summation, we need three extra variables corresponding to each carry term, say X1, X2 and X3. Although the carry term which comes by adding ‘T’ is going to be the same as F, we prefer to take them as a different variable for more straightforward perception. That is fine as long as we include it in our constraints. So our variables should be {T, W, O, F, U, R, X1, X2, X3}. Domain should be {0, 1,2,....,9}. Constraints should be as-

F = X3

O + O = R + 10*X1

Likewise for other terms and carries too.

Note the constraints here. One constraint involves three variables. And that would be the case for other terms too. Therefore, this problem is not a binary CSP, and we would need an auxiliary node while constructing the constraint graph for the formulation. For example, the first constraint would connect two nodes while each summation constraint would connect three nodes and the constraint that all the distinct letters should be different would connect all the non-carry-term variables. 

Another example that can be formulated as a CSP is the Sudoku. The formulation is done by considering each blank cell as a variable that can take any value in the domain- {1,2,3,4,5,6,7,8,9} and the standard rules for solving a sudoku puzzle being the constraints (that is, each column, row and block being nine-way different). However, setting an appropriate set of constraints is highly configurable. For example, instead of what’s described previously, we can have several pairwise inequalities of cells as a constraint. In short, anything as long as it solves the problem correctly.

Other than solving puzzles, CSPs are applied in many practically appealing problems as well. One such example is the Waltz algorithm. This algorithm can perceive 3D objects from their line drawing. As in, it can tell whether a polyhedron is concave or convex. All this just by looking at the line drawing! The algorithm takes vertices as the variables with possible values corresponding to ‘inward’ or ‘outward’. Each adjacent vertex constrains each other and putting all that together the algorithm comes up with a realisable 3D interpretation of the object.

We have seen several examples of CSPs in the previous section. All the problems had a variety in the type of variables, domain and constraints. That brings us to discuss the sorts of CSPs we may come across. Based on the type of variables, CSP can be classified into two categories- CSP with discrete variables and CSP with a continuous variable. Within them, they can be subclassified as CSP with discrete variables and finite domain or infinite domain. Till now, we have seen only CSPs with discrete variables and finite domain. There can be problems with continuous variables too, like some problems with timestamps as variables. Besides this, CSP can have a variety of constraints also. We have seen problems with binary constraints (the map-colouring problem) and high-order constraints (the N-Queen problem). There can be problems with unary constraints too. But it boils down to restricting the domain of the variable to one possible value.

We have discussed that in CSP, we are not concerned with the path that leads to the solution state. Instead, we just want to get to the solution state. Since in CSPs, it is generally the case that there is more than one combination of variables that satisfies the constraints. For example, suppose we came up with a valid assignment of colour in the map-colouring problem. Now if we interchange any two colours in the solution configuration, it still would count as a legit solution. This happened because we just wanted to ‘SATISFY’ the constraints using any formation. If only, say, one colour was costlier than the other and cheaper than yet another, we would get a filtered solution. Because here we have enforced a filter constraint that would select a solution optimised on the cost. That would make our constraint satisfaction problem into the constraint optimisation problem.

There are countless real-world problems whose formulation into CSPs is applicable—for example, assigning classes to teachers, timetabling, transportation scheduling and many more. Other than this, the problem of coming up with a suitable configuration of transistors and other electronic components on a small-sized board can be modelled as a CSP.

As far as solving a CSP is concerned, we can pose it as a search task, which should not be tough considering we are searching for a combination of values for our variable at the end of the day. In order to present it as a search problem, we need to have an initial state, a goal test and a successor function. We originally have a set of unassigned variables and we want to discover a set of values for them such that our constraints are fulfilled. On these grounds, our initial state becomes a set of unassigned variables and our goal test becomes a set of values for our variables that parallelly satisfies all the constraints. Depending on our strategy, we can have a suitable successor function that assigns/reassigns the values to our variables.

Let’s try to do the dry implementation and see how standard graph search methods like BSF and DFS perform. First up is the BFS algorithm. For the said purpose, we are taking the map-colouring problem as an example. Let’s go with the example discussed earlier. We have a map of Australia which has seven states, namely, WA, NT, Q, SA, V, T and NSW. We need to come up with a combination of three colours filled in these seven states that no adjacent state is of the same colour. If we go on to construct the search tree, on root we would have a set of unassigned variables. For the second level, we would assign one colour to any one of the states and leave others unassigned. In this manner the branching factor of the root node becomes seven times three, that is twenty-one children. Having done with the second level, the control would move on to explore the next level which would have six children of each of those twenty-one nodes (as one variable has already been assigned a value). Just so that you are not lost, it is BFS, we are at the third level yet and the tree is already splattering with 400 nodes. This process would continue on until all the variables are assigned with a value. Which is going to happen at the eighth level. And it is the eighth level only where we would get our solution. It can’t get any worse.

BFS turned out miserable at solving CSPs. Let’s see how DFS does. DFS explores the tree in level-wise fashion. At first, it assigns a value to one of the variables, then the other, so on and so forth up till the last variable. It might not find the perfect solution at once but it is at least getting to the level where the solution resides straight away (which BFS failed to do).

By this, it is clear that naive DFS is the better algorithm than naive BFS for solving CSPs. In the future, if we would like to improve upon either of these algorithms, DFS sounds more promising than the other. To put perspective to it using Australia map-colouring problem, in case of DFS we are sure to find a solution in the first branch of the second level while in case of BFS we explore each branch at second level itself that makes the occurrence of the solution from any branch ineffectual. Using DFS, we have reduced the computation by at least 21 times at the first level itself!

Let’s try to build upon DFS to improve the search. One obvious proposal should be to assign only satisfying values unlike in naive DFS where we were constructing a tree based on all the possible combinations regardless of the constraints of the problem. One convincing approach is to use the backtracking algorithm. It is yet another uninformed search algorithm used to solve CSPs. There are two key ideas that if understood, is half work done. We have seen in DFS that it explores branches which are effectively redundant. Whether first assigns white to WA then black to NT or first assigns black to NT then white to WA, both are the same thing. It can be dealt with by giving the variable an order that one should be assigned a value before the other and after some other. Next is the fact that we are not checking the constraints after each assignment which are putting us into wrong branches altogether. If the first two assignments don’t fulfil the requirements then the whole exploration of that branch is a complete waste. So we can improve the algorithm by putting a check after each assignment. Giving a succinct account of the said algorithms- we saw DFS was first doing the complete assignment and then only checking if the constraints are satisfied or not. As a result, we end up doing unfruitful work. In backtracking, we check for the constraint satisfaction at each assignment hence minimising the number of searches. In a nutshell, it can be claimed that-

Backtracking = DFS + Ordering the variables + Constraint satisfaction check

There are some general ideas that we can incorporate while designing an algorithm for solving the CSPs. First is ordering the variable just like we did in the previous section with backtracking. Second is filtering out the combination of assignments whose failure is obvious. We have seen how a CSP can be represented as the constraint graphs. The third idea involves leveraging the structure in the problem using the constraint graph. We are going to improve the backtracking algorithm by implementing these ideas on it. First, we will look at filtering in the context of backtracking.

There are several ways of filtering. First, we will see the forward checking approach. In forward checking, we tend to shrink the domain of unassigned variables with each subsequent assignment. Propagating in this manner, if we come across a state which does not fit the constraints, we ditch that path and backpropagate to the alternatives. Again, using the Australia map-colouring problem, initially with unassigned variables, every state would have equal and complete domain (i.e {red, green, blue}). Suppose we first assign the value red to the variable corresponding to the state of West Australia (that is, WA). As a consequence of that, the bordering states- Northern Territory (NT) and South Australia (SA) can never have the same value as that of West Australia (WA). So we shrink the domains of these two variables altogether. Processing in this way, if the domain of some variable becomes empty, we stop and backpropagate because such situations would not yield any solution. Putting it another way, we are not waiting for a state to violate the constraint. We rectify the occurrence of such states as we realise it (as seen in the example). But what if the domains of two adjacent variables shrink to one same variable? Simple backpropagation would not propagate until the domain of some variable is completely empty. But in the said situation it is reasonable to ditch that pursuit. We will fix this problem using consistent arcs.

While filtering, one might get an illusion that we are doing extra computation by foreseeing the future for omitting the bad search states. But as we have seen previously, with each level the number of nodes are going to increase. And if we could invest a little computation for cross-checking the nodes, we can avoid traversing a significant section of the tree. The computational power that we save by performing filtering is manyfold higher than the amount spent in related computations.

We briefly discussed the implications of constraint graphs in solving CSPs. Before we see how it is done, we need to understand the significance of consistency of an arc. While constructing the graph, we depicted the notion of variables being constrained using arcs. For example, in Australia map-colouring problem, we drew an arc between WA and NT as they share a common boundary. Suppose we assign a value (say red) to WA. By doing so, we made the value of NT to constrain to the value of WA (that is red). That can be perceived as a directed arc from NT to WA. According to the definition, an arc from some variable A to variable B is said to be consistent if and only if all the values in the domain of the variable at the tail of the arc satisfactorily (from constraint point of view) fits against the fixed value of the variable at the head. So for being consistent, the domain of the variable NT has to shrink down by one so that all the values in the new domain satisfy the constraints of the problem. To cement the concept, consider the variables WA and Q. They don’t share any boundaries. So by assigning a value to WA, it would not affect the domain of the variable Q. We do similar pairwise consistency checking with all the variables iteratively. Note that if the domain of variable changes in an attempt to make an arc consistent, it would affect the consistency of all the arcs from that variable to all the other variables. That’s why this process is needed to be repetitively performed whenever there is any domain change or variable assignment until all the arcs are consistent.

Backpropagation with enforced arc consistency is not impeccable though. As in, it does not detect all failures. It only detects some possible failures. These limitations originate from the fact that it checks only pairwise consistency. If our problem has higher-order constraints then there might be cases where the algorithm labels a set of arcs as consistent while actually they are not.

There is yet another technique which can speed things up for backpropagation. This technique helps the algorithm to decide what would be the best choice of the variable for assignment among all the possible options. The first approach we adopt is the Minimum Remaining Values. According to this, we do the next assignment to that variable which has the smallest domain. And if there are several such variables with the smallest domain size, we assign that variable that results in a maximum shrinkage in the domain size. We choose the most constrained variable for the subsequent assignment because if we chose a lesser constrained one then the possibility of pursuit getting failed in future increases.

All the search algorithms and techniques that we have seen up until now had several things in common with them. In every one of them, we used to have a start state and a goal state/test which we wished to reach/accomplish. In all the examples we have seen in the course of understanding these algorithms and techniques, we had our agent to simulate the paths from the source to the goal and come up with a suitable solution, (path) which then it implements. Now we are going to look at local search techniques. It is different from what we have already seen in a few ways. In local search, the agent need not simulate anything. Since it is not simulating anything, it can be inferred that it does not need any memory and also, in the end, we would not get a path. It just needs to know its current state and based on what’s around it, it goes to the state which it seems fit according to the criteria of the original problem. It can be imagined that our state has some value associated with it (just like heuristic value) which tells the agent about the soundness of that state. While talking about CSPs, we briefly touched upon constraint optimization. Local search is a technique used in such scenarios. Just to put everything about the local search technique in place- the agent knows only about its current state, the agent can move to its neighbouring states only, the agent does not keep a track of the path it is taking while exploring. These characteristics put this technique in advantage as it has very little memory requirements hence it would give us the solution regardless of how big the search space is. If our problem is only considerate about optimizing a function, (either minimising or maximising) the local search technique is recommended. Since the algorithm is onto the optimisation task, each state can be deemed to be a solution (just not the best one). We would now study a whole series of such algorithms which adopt the idea of local search.

First algorithm which we take interest in is called Hill-climbing (greedy local search). Hill climbing is a simple algorithm where we are given a state-space, (so we have something to traverse within) a start state and a function of states that the algorithm is required to optimise (either maximise or minimise). Assuming a maximisation task, It evaluates the function value at its current state and its immediate neighbouring states and then performs transition to that state which has the maximum objective function value. If a situation occurs where there are multiple states with the same value, then any one of them is chosen at random. On doing such decisive transitions repetitively, it can be said that asymptotically, the agent converges to the local optima of the objective function. On that thought, it is said that the Hill climbing algorithm is analogous to “Climbing a mountain in a thick fog with amnesia.”. In future, we are going to study algorithms with its concept build upon this algorithm.

Note where I wrote reaching a ‘local optima’- depending on from where we start exploring, we might end up at different maxima/minima which are not the actual optima of the objective function. It happens when the objective function fluctuates in the range of interest and since the agent can move in the direction of fulfilment, it may get stuck in a local optimum.

Let’s revisit the N-Queens problem and try to pose it as a constraint optimization problem. First and foremost, we need to come up with an objective function for it which we can optimise in order to come up with an arrangement of N queens on an NxN chessboard such that no two queens attack each other. Come to think of it, we can use ‘number of queens attacking each other’ narrative itself as an objective function. We would like it to be zero in the ideal case. Our state space, in this case, is the arrangements of N queens on the chessboard. As exploration criteria, we also need to come up with a successor function. Just to reduce the randomness (number of possible states), let’s constrain our queen to move along the same pre-allotted column only. As mentioned before also, in local search techniques, every state is a solution. It is the matter of the value of the objective function at that state that ruminates the effectiveness of that solution. This thought resonates with the idea of ‘stuck in a local optima’ too.

Here are some statistics- if hill climbing is performed on 8- queens problem with randomly generated starting states, 14% of the time it solved it with optimal solution that too in 4 steps on average. While on other times it got stuck in a local minimum and it took on average 3 steps to converge to the local minimum. Considering the fact that there are millions of possible start states (8^8 to be exact), it is appreciable that the algorithm is able to provide a solution within 3-4 steps on average.

There are several limitations of the hill climbing algorithm which are needed to be dealt with. One is that it gets stuck on local minima/maxima. Second is, other than local optima, it may get stuck at a plateau. That is, there might be cases where the neighbouring value of the objective function is the same as the current value. This limitation is more because of the successor function. For example, in N-Queens problem, suppose we constrain the queens to column wise movement only but our objective function is giving an optimal value at a diagonal state. Since the movement of that sort is not allowed by constraints. This is called diagonal ridge.

We can solve the problem due to the occurrence of plateaus by allowing the agent to hold the value of objective function in the memory for multiple states. In addition to that, we have to allow the agent for sideways movement to concurrent states in a hope that eventually it will be able to escape the shoulder (the plateau). The approach is called the tabu search. Let’s briefly look into that to give a structure to the idea. In tabu search, we maintain a queue of fixed length (depending on how long of a plateau you are anticipating on the given objective function) where we add the most recent state to it and once the queue is full, we dequeue the oldest state from it. Moreover, we never revisit a state which is currently tabu’ed. As a result, as the queue grows in size, the state becomes essentially non-redundant. I am taking the liberty to reiterate that we are using the queue while at a plateau only.

To provide a perspective on how effective it can be- on the 8-queens problem, if this tabu search with queue size of 100 is implemented, the success rate increases from 14% to 94% on cost of increased number of steps (21 for optimal solution and 64 otherwise). Looking at the statistics, it is clear that tabu search can improve the performance of simple hill climbing algorithm significantly. But with a limited size of the queue there is not much that one can do when there is a wide plateau. An alternative is to use exhaustive search technique on a plateau. We can choose any search method. Let’s say we choose BFS (recommended). So going with this approach, the agent performs usual hill climbing up until it encounters a plateau. At the plateau, BFS is performed for searching the foot of the hill ahead. In this manner, a plateau on a hill can be escaped. Because it uses exhaustive search technique, which, unlike the usual algorithms of local search technique, don’t shy away from consuming as much space as it wants, this approach lies more on the middle ground between local and systematic search.

The problem of local minima can be combated by introducing some randomness in our approach. We will see two major ways to do this job- random walk and random restart (and the combination of both). Here we try to implement the simple hill climbing with a stochastic touch. Let’s first see it with a random walk. Let’s call it the stochastic hill climbing algorithm using random walk. A greedy algorithm would go for the best successor, while a stochastic algorithm would choose one of the many better successors. And that is what the stochastic hill climbing algorithm does. It assigns a probability (say p) to the successors based on the value of objective function at them. After getting the probability, we choose the best successor with a probability of p and a random successor with a probability of 1-p. If p is high (>0.5), hill climbing would go greedy and if p is low (<0.5) it will go in randomized fashion.

Alternatively, we can have a stochastic hill climbing algorithm with random restart. In this approach we randomly select a start state and get to an optimal state and store it in the memory and restart the whole process again with a different initial state. We repetitively do this process several times and choose the best state among all the repetitions. It has been by far one of the best in all the algorithms in local search algorithms.

Moving on from hill climbing algorithm and its variants, let’s see another local search technique known as Simulated Annealing. This algorithm is inspired from the physical process of annealing where on providing heat to a material, it gains energy as a result its physical ( adn chemical) state changes and once the material is cooled down, the particle settles down into a more stable configuration. Similarly, we provide our agent with temperature by allowing it to explore a random successor state based on certain criteria and hope that asymptotically, its state would end up onto the global optima. It is not much different from the usual hill climbing in the sense that we explore the successors on the basis of their quality. But the catch is, instead of picking and transitioning to the best state right away, we pick a successor at random and evaluate the change in the value of the objective function. If it is positive, then move to that state, otherwise mov to that state with a probability proportional to the difference value. This ensures the worse moves are taken the least often. The algorithm allows the step size to be modulated as per the need. This approach is highly time taking but eventually converges the agent toward the global optima. That’s why it is not used until or unless essentially required. This algorithm is widely used in VLSI layout problems because there, coming up with a globally optimal solution is crucial.

Local beam search is yet another local search algorithm of sorts. In this, we do the usual hill climbing tasks with k states at once. Initially we select k random start states. Next, we enlist all the successors states of all the k states and choose the k best among them, so on and so forth until the goal is reached. Other than the obvious limitations like all k states ending up at the same local optima, one other drawback could be, suppose if all the k best successor states come from a single state. That’d kill the diversity and hence the whole purpose of the local beam search. To avoid such consequences, we can induce some randomness here too. For example, instead of choosing the best k successor states, we can choose them probabilistically, being biased towards the best ones. 

In the local beam search algorithm, the analogy with the biological process of natural selection is quite obvious. As we first had k states which provided us with several options among which we chose the best k.



Taking forward the idea of natural selection, we come to a whole different class of algorithms known as the genetic algorithms. In that, one or more states give rise to one or more successor states. There are some components of a genetic algorithm. First is the fitness function- that tells the survival probability of a state. Essentially, if the objective function has a higher value at a state, then its fitness function would be higher. And more is fitness better are the chances of survival. Then comes the crossover process- in this, based on the fitness of the states, pairs are chosen at random from a set of randomly picked k number of initial states, then we take several random characteristics of each state of the pair and come up with a completely new state. The third is random mutation- where we randomly change some characteristic of that newly formed state. For example, let’s represent the states of the N-queens problem using a string of N integers each representing the row index at which the ith queen is. We first choose k random states represented in the aforementioned manner. Then we compute the fitness value of each state (which can be the number of non-attacking queens in that configuration on the chessboard). Then we do a ‘random selection with repetition’ from those k states to come up with k states derived from them. So at this stage, one state may be present twice or more. Note that while selection, we have been biased towards the fitter states. In the next step, we do the crossover. For this, we pair each state and randomly partition each member of each pair and make two states by taking and appending one part from each paired state. That makes our new population. But we are not done making the successor states yet. In the final step, we do random mutation. In this, we randomly change one value from the configuration string as in the N-Queens problem. Up until now, we have never seen an algorithm which was not moving the direction of improving objective function. But here, the strategic induction of randomness allows the algorithm to make intelligent jumps so that we can explore the states which are not in immediate neighbourhoods. One negative point is that there is a lot of uncertainty in the algorithm. Which is evident by the fact that with each iteration, one may get different results. That makes reproducing a set of results practically impossible.  



We have seen hill climbing. There we had discretized values to propagate through. We can do optimization tasks on continuous functions too. That brings us to the topic of gradient descent/ascent. In the case of optimisation of a continuous function, we can explicitly discretise the function and perform a simple hill-climbing algorithm (or variant). 

If we have to minimise the objective function in continuous space, we perform gradient descent otherwise gradient ascent. Suppose we have to minimise over a set of variables x1,x2,x3...xn. For this purpose, we first choose a point to start with, then compute the gradient at that point with respect to all the variables and then update our state (point) in the direction of the gradient. The amount of movement we take in that direction depends on a hyperparameter lambda. We would like to keep that high initially so that we make bigger moves in the beginning. Then we reduce its value as we go down so that sensitivity can be improved.



Refs:

https://en.wikipedia.org/wiki/Four_color_theorem

Lecture Slides and recordings

https://www.youtube.com/watch?feature=player_embedded&v=OGcG4jSKOVA

https://www.youtube.com/watch?feature=player_embedded&v=hegL0V4ckco

https://www.youtube.com/watch?feature=player_embedded&v=LajAWn51HmE

https://www.youtube.com/watch?v=xcWGAjPG3hg